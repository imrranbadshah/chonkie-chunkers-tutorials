{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide to Chonkie Chunkers\n",
    "\n",
    "## Master All 9 Chunking Strategies for RAG Applications\n",
    "\n",
    "Welcome to this comprehensive tutorial on Chonkie - the lightweight, high-performance chunking library for Retrieval-Augmented Generation (RAG) systems!\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **9 Different Chunking Strategies**: From basic token splitting to advanced LLM-powered chunking\n",
    "- **Google Gemini Integration**: Using Gemini embeddings for semantic chunking\n",
    "- **Practical Examples**: Real-world applications with technical docs and research papers\n",
    "- **Performance Comparisons**: Side-by-side analysis of all chunkers\n",
    "- **Best Practices**: How to choose the right chunker for your use case\n",
    "\n",
    "### Tutorial Structure (2-2.5 hours)\n",
    "\n",
    "1. Introduction & Setup (15 min)\n",
    "2. Foundation Chunkers (20 min) - Token, Sentence, Recursive\n",
    "3. Specialized Chunkers (20 min) - Table, Code\n",
    "4. Semantic Chunkers (25 min) - Semantic, Late, Neural\n",
    "5. Advanced Chunker (15 min) - Slumber (LLM-powered)\n",
    "6. Comparative Analysis (15 min)\n",
    "7. Best Practices (10 min)\n",
    "8. Exercises & Next Steps (5 min)\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction & Setup\n",
    "\n",
    "### 1.1 What is Chonkie?\n",
    "\n",
    "Chonkie is a production-ready text chunking library designed specifically for RAG applications. It provides:\n",
    "\n",
    "- **9 specialized chunkers** for different document types\n",
    "- **Local processing** - your data never leaves your infrastructure\n",
    "- **High performance** - optimized for speed and efficiency\n",
    "- **Thread-safe** - suitable for concurrent processing\n",
    "- **Flexible embeddings** - works with OpenAI, Gemini, Sentence Transformers, and more\n",
    "\n",
    "### Why Chunking Matters\n",
    "\n",
    "In RAG systems, chunking directly impacts:\n",
    "- **Retrieval Quality**: Better chunks = more relevant context\n",
    "- **Embedding Quality**: Semantic coherence improves embeddings\n",
    "- **Generation Accuracy**: Relevant context leads to better answers\n",
    "- **System Performance**: Efficient chunking reduces latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Installation Verification\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import chonkie\n",
    "    print(f\"Chonkie version: {chonkie.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: Chonkie not installed. Please run: pip install -r requirements.txt\")\n",
    "    \n",
    "# Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\\nAll required libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Load Environment Variables\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not GEMINI_API_KEY or GEMINI_API_KEY == \"your_api_key_here\":\n",
    "    print(\"WARNING: Gemini API key not configured!\")\n",
    "    print(\"Please set your GEMINI_API_KEY in the .env file\")\n",
    "    print(\"Get your key from: https://makersuite.google.com/app/apikey\")\n",
    "else:\n",
    "    print(\"Gemini API key loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Setup Google Gemini Embeddings\n",
    "from chonkie import GeminiEmbeddings\n",
    "\n",
    "# Initialize Gemini embeddings\n",
    "try:\n",
    "    embeddings = GeminiEmbeddings(\n",
    "        model=\"gemini-embedding-exp-03-07\",\n",
    "        api_key=GEMINI_API_KEY,\n",
    "        task_type=\"SEMANTIC_SIMILARITY\"\n",
    "    )\n",
    "    \n",
    "    # Test embeddings\n",
    "    test_text = \"Hello, this is a test embedding\"\n",
    "    test_vector = embeddings.embed(test_text)\n",
    "    \n",
    "    print(f\"Embedding dimension: {len(test_vector)}\")\n",
    "    print(f\"Sample values: {test_vector[:5]}\")\n",
    "    print(\"\\nGemini embeddings configured successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up Gemini embeddings: {e}\")\n",
    "    print(\"Continuing with tutorial - semantic chunkers will not work without valid API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Load Sample Data\n",
    "# Load technical documentation\n",
    "with open(\"../data/sample_technical_doc.txt\", \"r\") as f:\n",
    "    technical_doc_text = f.read()\n",
    "\n",
    "# Load research paper\n",
    "with open(\"../data/sample_research_paper.txt\", \"r\") as f:\n",
    "    research_paper_text = f.read()\n",
    "\n",
    "# Load code sample\n",
    "with open(\"../data/sample_code.py\", \"r\") as f:\n",
    "    code_text = f.read()\n",
    "\n",
    "# Load table sample\n",
    "with open(\"../data/sample_table.md\", \"r\") as f:\n",
    "    table_text = f.read()\n",
    "\n",
    "print(f\"Technical Doc: {len(technical_doc_text):,} characters\")\n",
    "print(f\"Research Paper: {len(research_paper_text):,} characters\")\n",
    "print(f\"Code Sample: {len(code_text):,} characters\")\n",
    "print(f\"Table Sample: {len(table_text):,} characters\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n--- Technical Doc Preview ---\")\n",
    "print(technical_doc_text[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Helper Functions\n",
    "\n",
    "def display_chunks(chunks, max_display=3, show_metadata=True):\n",
    "    \"\"\"Display chunks with metadata.\"\"\"\n",
    "    print(f\"\\nTotal chunks: {len(chunks)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks[:max_display]):\n",
    "        print(f\"\\nChunk {i+1}:\")\n",
    "        print(f\"Text: {chunk.text[:200]}...\")\n",
    "        if show_metadata and hasattr(chunk, 'token_count'):\n",
    "            print(f\"Token count: {chunk.token_count}\")\n",
    "        if show_metadata and hasattr(chunk, 'start_char'):\n",
    "            print(f\"Position: {chunk.start_char} - {chunk.end_char}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    if len(chunks) > max_display:\n",
    "        print(f\"\\n... and {len(chunks) - max_display} more chunks\")\n",
    "\n",
    "\n",
    "def visualize_chunk_sizes(chunks, title=\"Chunk Size Distribution\"):\n",
    "    \"\"\"Create bar chart of chunk token counts.\"\"\"\n",
    "    token_counts = [chunk.token_count if hasattr(chunk, 'token_count') else len(chunk.text) \n",
    "                   for chunk in chunks]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(token_counts, bins=20, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Token Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{title}\\nDistribution')\n",
    "    plt.axvline(np.mean(token_counts), color='red', linestyle='--', label=f'Mean: {np.mean(token_counts):.0f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(token_counts, vert=True)\n",
    "    plt.ylabel('Token Count')\n",
    "    plt.title(f'{title}\\nBox Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Mean: {np.mean(token_counts):.1f}\")\n",
    "    print(f\"  Median: {np.median(token_counts):.1f}\")\n",
    "    print(f\"  Std Dev: {np.std(token_counts):.1f}\")\n",
    "    print(f\"  Min: {min(token_counts)}\")\n",
    "    print(f\"  Max: {max(token_counts)}\")\n",
    "\n",
    "\n",
    "def compare_chunkers(text, chunkers_dict, sample_size=1000):\n",
    "    \"\"\"Compare multiple chunkers on same text.\"\"\"\n",
    "    results = {}\n",
    "    sample_text = text[:sample_size] if len(text) > sample_size else text\n",
    "    \n",
    "    for name, chunker in chunkers_dict.items():\n",
    "        start_time = time.time()\n",
    "        chunks = chunker.chunk(sample_text)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        token_counts = [chunk.token_count if hasattr(chunk, 'token_count') else len(chunk.text) \n",
    "                       for chunk in chunks]\n",
    "        \n",
    "        results[name] = {\n",
    "            'num_chunks': len(chunks),\n",
    "            'avg_size': np.mean(token_counts),\n",
    "            'std_dev': np.std(token_counts),\n",
    "            'time': elapsed\n",
    "        }\n",
    "    \n",
    "    df = pd.DataFrame(results).T\n",
    "    return df\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Foundation Chunkers\n",
    "\n",
    "These are the basic, high-performance chunkers that work well for most use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 TokenChunker - Fixed-Size Token Windows\n",
    "\n",
    "**Best for:** General purpose, consistent chunk sizes, high-throughput applications\n",
    "\n",
    "**How it works:** Splits text into fixed-size token chunks with configurable overlap\n",
    "\n",
    "**Pros:**\n",
    "- Very fast (12,000 chunks/sec)\n",
    "- Predictable chunk sizes\n",
    "- No external dependencies\n",
    "- Works with any text\n",
    "\n",
    "**Cons:**\n",
    "- May break sentences mid-thought\n",
    "- No semantic awareness\n",
    "- Can fragment important context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import TokenChunker\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Create TokenChunker\n",
    "token_chunker = TokenChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=512,      # Max tokens per chunk\n",
    "    chunk_overlap=128    # Overlapping tokens between chunks\n",
    ")\n",
    "\n",
    "# Chunk technical documentation\n",
    "token_chunks = token_chunker.chunk(technical_doc_text)\n",
    "\n",
    "# Display results\n",
    "display_chunks(token_chunks)\n",
    "visualize_chunk_sizes(token_chunks, \"TokenChunker Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Experiment with Parameters\n",
    "\n",
    "Try different chunk_size and chunk_overlap values:\n",
    "- chunk_size: 256, 512, 1024\n",
    "- chunk_overlap: 0, 64, 128, 256\n",
    "\n",
    "Observe how they affect the number and size of chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experimentation here\n",
    "# Example:\n",
    "# token_chunker_small = TokenChunker(tokenizer=tokenizer, chunk_size=256, chunk_overlap=64)\n",
    "# small_chunks = token_chunker_small.chunk(technical_doc_text[:2000])\n",
    "# display_chunks(small_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SentenceChunker - Sentence Boundary Preservation\n",
    "\n",
    "**Best for:** Q&A systems, semantic search, preserving complete thoughts\n",
    "\n",
    "**How it works:** Accumulates complete sentences until reaching token limit\n",
    "\n",
    "**Pros:**\n",
    "- Preserves sentence integrity\n",
    "- Better for question answering\n",
    "- Still quite fast (8,500 chunks/sec)\n",
    "\n",
    "**Cons:**\n",
    "- Variable chunk sizes\n",
    "- May group unrelated sentences\n",
    "- Depends on sentence detection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SentenceChunker\n",
    "\n",
    "# Create SentenceChunker\n",
    "sentence_chunker = SentenceChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=512,\n",
    "    min_sentences_per_chunk=2  # Ensure at least 2 sentences per chunk\n",
    ")\n",
    "\n",
    "# Chunk research paper\n",
    "sentence_chunks = sentence_chunker.chunk(research_paper_text)\n",
    "\n",
    "display_chunks(sentence_chunks)\n",
    "visualize_chunk_sizes(sentence_chunks, \"SentenceChunker Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TokenChunker vs SentenceChunker\n",
    "comparison_text = research_paper_text[:5000]\n",
    "\n",
    "token_chunks_comp = token_chunker.chunk(comparison_text)\n",
    "sentence_chunks_comp = sentence_chunker.chunk(comparison_text)\n",
    "\n",
    "print(\"Comparison on same text:\")\n",
    "print(f\"\\nTokenChunker: {len(token_chunks_comp)} chunks\")\n",
    "print(f\"SentenceChunker: {len(sentence_chunks_comp)} chunks\")\n",
    "\n",
    "print(\"\\n--- TokenChunker First Chunk ---\")\n",
    "print(token_chunks_comp[0].text[:300])\n",
    "\n",
    "print(\"\\n--- SentenceChunker First Chunk ---\")\n",
    "print(sentence_chunks_comp[0].text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RecursiveChunker - Hierarchical Splitting\n",
    "\n",
    "**Best for:** Markdown, structured documents, technical documentation\n",
    "\n",
    "**How it works:** Uses hierarchy of separators (paragraphs → sentences → words)\n",
    "\n",
    "**Pros:**\n",
    "- Respects document structure\n",
    "- Excellent for markdown\n",
    "- Customizable separators\n",
    "- Good balance of speed and quality\n",
    "\n",
    "**Cons:**\n",
    "- Less effective on unstructured text\n",
    "- Requires well-formatted documents\n",
    "- Performance depends on separator choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import RecursiveChunker\n",
    "\n",
    "# Create RecursiveChunker\n",
    "recursive_chunker = RecursiveChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=128,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Priority order\n",
    ")\n",
    "\n",
    "# Chunk technical documentation (has good structure)\n",
    "recursive_chunks = recursive_chunker(technical_doc_text)  # Can also use __call__\n",
    "\n",
    "display_chunks(recursive_chunks)\n",
    "visualize_chunk_sizes(recursive_chunks, \"RecursiveChunker Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate hierarchy preservation\n",
    "print(\"First chunk shows hierarchy preservation:\")\n",
    "print(recursive_chunks[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Specialized Chunkers\n",
    "\n",
    "These chunkers are optimized for specific content types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 TableChunker - Tabular Data Handling\n",
    "\n",
    "**Best for:** Markdown tables, CSV data, structured tabular content\n",
    "\n",
    "**How it works:** Splits tables by rows while preserving headers\n",
    "\n",
    "**Pros:**\n",
    "- Preserves table structure\n",
    "- Maintains header context\n",
    "- Handles large tables efficiently\n",
    "\n",
    "**Cons:**\n",
    "- Only works with tables\n",
    "- Requires markdown format\n",
    "- May create very large chunks for wide tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import TableChunker\n",
    "\n",
    "# Create TableChunker\n",
    "table_chunker = TableChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=512\n",
    ")\n",
    "\n",
    "# Chunk table data\n",
    "table_chunks = table_chunker.chunk(table_text)\n",
    "\n",
    "print(f\"Table split into {len(table_chunks)} chunks\")\n",
    "print(\"\\nFirst chunk (with headers):\")\n",
    "print(table_chunks[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 CodeChunker - Source Code Chunking\n",
    "\n",
    "**Best for:** Source code, API documentation, code repositories\n",
    "\n",
    "**How it works:** Uses Abstract Syntax Tree (AST) to chunk at logical boundaries\n",
    "\n",
    "**Pros:**\n",
    "- Preserves function/class boundaries\n",
    "- Maintains code structure\n",
    "- Language-aware chunking\n",
    "- Excellent for code search\n",
    "\n",
    "**Cons:**\n",
    "- Only works with code\n",
    "- Requires valid syntax\n",
    "- Language-specific (Python, JavaScript, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import CodeChunker\n",
    "\n",
    "# Create CodeChunker\n",
    "code_chunker = CodeChunker(\n",
    "    language=\"python\",\n",
    "    tokenizer_or_token_counter=tokenizer,\n",
    "    chunk_size=2048,\n",
    "    include_nodes=True  # Include AST node information\n",
    ")\n",
    "\n",
    "# Chunk Python code\n",
    "code_chunks = code_chunker.chunk(code_text)\n",
    "\n",
    "print(f\"Code split into {len(code_chunks)} chunks\")\n",
    "print(\"\\n--- First Code Chunk ---\")\n",
    "print(code_chunks[0].text[:500])\n",
    "\n",
    "# Show function boundaries\n",
    "if hasattr(code_chunks[0], 'node_type'):\n",
    "    print(f\"\\nNode type: {code_chunks[0].node_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Semantic/Embedding-Based Chunkers\n",
    "\n",
    "These chunkers use embeddings to identify semantic boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 SemanticChunker - Topic Boundary Detection\n",
    "\n",
    "**Best for:** Multi-topic documents, preserving topical coherence\n",
    "\n",
    "**How it works:** Uses embeddings to find topic boundaries based on semantic similarity\n",
    "\n",
    "**Pros:**\n",
    "- Excellent semantic coherence\n",
    "- Natural topic boundaries\n",
    "- Auto threshold detection\n",
    "- 23% better retrieval than fixed-size\n",
    "\n",
    "**Cons:**\n",
    "- Requires embedding API\n",
    "- Slower (450 chunks/sec)\n",
    "- API costs\n",
    "- Variable chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker\n",
    "\n",
    "# Create SemanticChunker with Gemini embeddings\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embedding_model=embeddings,  # Using our Gemini embeddings\n",
    "    threshold=\"auto\",  # Automatically determine threshold\n",
    "    chunk_size=512,\n",
    "    min_sentences=1\n",
    ")\n",
    "\n",
    "# Chunk research paper (has multiple topics)\n",
    "semantic_chunks = semantic_chunker.chunk(research_paper_text)\n",
    "\n",
    "display_chunks(semantic_chunks)\n",
    "visualize_chunk_sizes(semantic_chunks, \"SemanticChunker Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show semantic similarity scores (if available)\n",
    "for i, chunk in enumerate(semantic_chunks[:5]):\n",
    "    print(f\"\\nChunk {i}: {len(chunk.text)} chars\")\n",
    "    if hasattr(chunk, 'similarity_score'):\n",
    "        print(f\"Similarity score: {chunk.similarity_score:.3f}\")\n",
    "    print(f\"Preview: {chunk.text[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare automatic vs manual threshold\n",
    "semantic_chunker_manual = SemanticChunker(\n",
    "    embedding_model=embeddings,\n",
    "    threshold=0.5,  # Manual threshold\n",
    "    chunk_size=512,\n",
    "    min_sentences=1\n",
    ")\n",
    "\n",
    "manual_chunks = semantic_chunker_manual.chunk(research_paper_text[:5000])\n",
    "\n",
    "print(f\"Auto threshold: {len(semantic_chunks)} chunks\")\n",
    "print(f\"Manual threshold (0.5): {len(manual_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LateChunker - Optimized for Retrieval\n",
    "\n",
    "**Best for:** Maximizing retrieval recall in RAG systems\n",
    "\n",
    "**How it works:** Generates document-level embeddings before chunking\n",
    "\n",
    "**Pros:**\n",
    "- Better retrieval recall\n",
    "- Richer contextual embeddings\n",
    "- Based on research\n",
    "\n",
    "**Cons:**\n",
    "- Slower (180 chunks/sec)\n",
    "- Higher memory usage\n",
    "- More expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import LateChunker\n",
    "\n",
    "# Create LateChunker\n",
    "late_chunker = LateChunker(\n",
    "    embedding_model=embeddings,\n",
    "    chunk_size=512,\n",
    "    context_size=2048  # Document-level context window\n",
    ")\n",
    "\n",
    "# Chunk technical documentation\n",
    "late_chunks = late_chunker.chunk(technical_doc_text)\n",
    "\n",
    "display_chunks(late_chunks)\n",
    "visualize_chunk_sizes(late_chunks, \"LateChunker Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 NeuralChunker - BERT-Based Semantic Shifts\n",
    "\n",
    "**Best for:** Complex documents with subtle topic variations\n",
    "\n",
    "**How it works:** Fine-tuned BERT model detects semantic boundaries\n",
    "\n",
    "**Pros:**\n",
    "- Detects subtle topic shifts\n",
    "- No API required (local model)\n",
    "- Learned representations\n",
    "- Good for academic papers\n",
    "\n",
    "**Cons:**\n",
    "- Requires GPU for speed\n",
    "- Slower (320 chunks/sec)\n",
    "- Higher memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import NeuralChunker\n",
    "\n",
    "# Create NeuralChunker\n",
    "neural_chunker = NeuralChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=512,\n",
    "    threshold=0.6  # Semantic shift threshold\n",
    ")\n",
    "\n",
    "# Chunk research paper\n",
    "neural_chunks = neural_chunker.chunk(research_paper_text)\n",
    "\n",
    "display_chunks(neural_chunks)\n",
    "visualize_chunk_sizes(neural_chunks, \"NeuralChunker Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Advanced Chunker - SlumberChunker\n",
    "\n",
    "### LLM-Powered Agentic Chunking\n",
    "\n",
    "**Best for:** Highest quality needs, books, research papers\n",
    "\n",
    "**How it works:** Uses LLM to intelligently determine chunk boundaries\n",
    "\n",
    "**Pros:**\n",
    "- Highest quality (92% recall@5)\n",
    "- Most intelligent boundaries\n",
    "- Understands context deeply\n",
    "\n",
    "**Cons:**\n",
    "- Very slow (8 chunks/sec)\n",
    "- Expensive ($45 per 1M chunks)\n",
    "- Requires LLM API\n",
    "- Not suitable for real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SlumberChunker\n",
    "\n",
    "# Create SlumberChunker\n",
    "slumber_chunker = SlumberChunker(\n",
    "    model=\"gemini-pro\",  # Can use Gemini\n",
    "    api_key=GEMINI_API_KEY,\n",
    "    chunk_size=512,\n",
    "    strategy=\"intelligent\"  # LLM decides boundaries\n",
    ")\n",
    "\n",
    "# Warning: This is slow and costs API credits!\n",
    "# Using only first 5000 characters\n",
    "slumber_chunks = slumber_chunker.chunk(research_paper_text[:5000])\n",
    "\n",
    "display_chunks(slumber_chunks)\n",
    "\n",
    "print(\"\\nNote: SlumberChunker is 140x slower than TokenChunker\")\n",
    "print(\"Use for highest quality needs where cost and latency are acceptable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Comparative Analysis\n",
    "\n",
    "Let's compare all chunkers side-by-side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Create all chunkers\n",
    "chunkers = {\n",
    "    \"Token\": token_chunker,\n",
    "    \"Sentence\": sentence_chunker,\n",
    "    \"Recursive\": recursive_chunker,\n",
    "    \"Semantic\": semantic_chunker,\n",
    "    \"Neural\": neural_chunker,\n",
    "}\n",
    "\n",
    "# Compare on sample text\n",
    "sample_text = research_paper_text[:10000]\n",
    "results_df = compare_chunkers(sample_text, chunkers)\n",
    "\n",
    "print(\"\\nComparative Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Number of chunks\n",
    "axes[0, 0].bar(results_df.index, results_df['num_chunks'])\n",
    "axes[0, 0].set_title('Number of Chunks')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average chunk size\n",
    "axes[0, 1].bar(results_df.index, results_df['avg_size'])\n",
    "axes[0, 1].set_title('Average Chunk Size')\n",
    "axes[0, 1].set_ylabel('Tokens')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Size variance\n",
    "axes[1, 0].bar(results_df.index, results_df['std_dev'])\n",
    "axes[1, 0].set_title('Size Variance (Std Dev)')\n",
    "axes[1, 0].set_ylabel('Tokens')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Processing time\n",
    "axes[1, 1].bar(results_df.index, results_df['time'] * 1000)\n",
    "axes[1, 1].set_title('Processing Time')\n",
    "axes[1, 1].set_ylabel('Milliseconds')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Best Practices & Use Cases\n",
    "\n",
    "### 7.1 Chunker Selection Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision matrix\n",
    "selection_guide = pd.DataFrame({\n",
    "    'Use Case': [\n",
    "        'Real-time high-throughput',\n",
    "        'Question answering',\n",
    "        'Markdown/structured docs',\n",
    "        'Source code',\n",
    "        'Data tables',\n",
    "        'Multi-topic documents',\n",
    "        'Maximum retrieval quality',\n",
    "        'Complex academic papers',\n",
    "        'Books/premium content'\n",
    "    ],\n",
    "    'Recommended Chunker': [\n",
    "        'TokenChunker',\n",
    "        'SentenceChunker',\n",
    "        'RecursiveChunker',\n",
    "        'CodeChunker',\n",
    "        'TableChunker',\n",
    "        'SemanticChunker',\n",
    "        'LateChunker',\n",
    "        'NeuralChunker',\n",
    "        'SlumberChunker'\n",
    "    ],\n",
    "    'Alternative': [\n",
    "        'SentenceChunker',\n",
    "        'SemanticChunker',\n",
    "        'SentenceChunker',\n",
    "        'RecursiveChunker',\n",
    "        'RecursiveChunker',\n",
    "        'NeuralChunker',\n",
    "        'SemanticChunker',\n",
    "        'SemanticChunker',\n",
    "        'SemanticChunker'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nChunker Selection Guide:\")\n",
    "print(\"=\" * 80)\n",
    "print(selection_guide.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Common Patterns\n",
    "\n",
    "#### Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing example\n",
    "documents = [\n",
    "    technical_doc_text[:2000],\n",
    "    research_paper_text[:2000],\n",
    "    \"Another document here...\"\n",
    "]\n",
    "\n",
    "# Process all documents at once\n",
    "batch_chunks = token_chunker.chunk_batch(documents)\n",
    "\n",
    "print(f\"Processed {len(documents)} documents\")\n",
    "for i, chunks in enumerate(batch_chunks):\n",
    "    print(f\"Document {i+1}: {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust chunking with fallback\n",
    "def safe_chunk(text, preferred_chunker, fallback_chunker):\n",
    "    try:\n",
    "        return preferred_chunker.chunk(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with preferred chunker: {e}\")\n",
    "        print(\"Falling back to simpler chunker\")\n",
    "        return fallback_chunker.chunk(text)\n",
    "\n",
    "# Example usage\n",
    "chunks = safe_chunk(\n",
    "    technical_doc_text,\n",
    "    preferred_chunker=semantic_chunker,\n",
    "    fallback_chunker=token_chunker\n",
    ")\n",
    "\n",
    "print(f\"Successfully chunked: {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Integration with RAG Pipeline\n",
    "\n",
    "Simple example of chunking → embedding → retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG pipeline\n",
    "def simple_rag_pipeline(documents, query, chunker, embedding_model, top_k=3):\n",
    "    # Step 1: Chunk documents\n",
    "    all_chunks = []\n",
    "    for doc in documents:\n",
    "        chunks = chunker.chunk(doc)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"Created {len(all_chunks)} chunks from {len(documents)} documents\")\n",
    "    \n",
    "    # Step 2: Embed chunks\n",
    "    chunk_texts = [chunk.text for chunk in all_chunks]\n",
    "    chunk_embeddings = embedding_model.embed_batch(chunk_texts)\n",
    "    \n",
    "    # Step 3: Embed query\n",
    "    query_embedding = embedding_model.embed(query)\n",
    "    \n",
    "    # Step 4: Find most similar chunks\n",
    "    similarities = []\n",
    "    for i, chunk_emb in enumerate(chunk_embeddings):\n",
    "        sim = np.dot(query_embedding, chunk_emb) / (\n",
    "            np.linalg.norm(query_embedding) * np.linalg.norm(chunk_emb)\n",
    "        )\n",
    "        similarities.append((i, sim))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top-k chunks\n",
    "    top_chunks = [all_chunks[i] for i, _ in similarities[:top_k]]\n",
    "    top_scores = [score for _, score in similarities[:top_k]]\n",
    "    \n",
    "    return top_chunks, top_scores\n",
    "\n",
    "# Example query\n",
    "query = \"What are the best practices for chunking in RAG systems?\"\n",
    "docs = [technical_doc_text, research_paper_text]\n",
    "\n",
    "retrieved_chunks, scores = simple_rag_pipeline(\n",
    "    docs, query, semantic_chunker, embeddings\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(\"\\nTop 3 Retrieved Chunks:\")\n",
    "for i, (chunk, score) in enumerate(zip(retrieved_chunks, scores)):\n",
    "    print(f\"\\n{i+1}. Similarity: {score:.3f}\")\n",
    "    print(f\"Text: {chunk.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Exercises & Next Steps\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. **Chunk Your Own Document**: Upload a document and try 3 different chunkers\n",
    "2. **Optimize Threshold**: Find the optimal threshold for SemanticChunker on your data\n",
    "3. **Speed Comparison**: Time all chunkers on a large document\n",
    "4. **Build Mini-RAG**: Create a simple retrieval system with your favorite chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Load your own document and experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Find optimal threshold\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, \"auto\"]\n",
    "test_text = research_paper_text[:5000]\n",
    "\n",
    "# Your code here to test different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Speed comparison\n",
    "# Time all chunkers on the full research paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **No one-size-fits-all**: Choose chunker based on your use case\n",
    "2. **Trade-offs matter**: Speed vs quality, consistency vs semantics\n",
    "3. **Start simple**: TokenChunker or SentenceChunker for most cases\n",
    "4. **Upgrade for quality**: Semantic chunkers when retrieval quality matters\n",
    "5. **Test with your data**: What works for others may not work for you\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Chonkie Documentation](https://docs.chonkie.ai/)\n",
    "- [Chonkie GitHub](https://github.com/chonkie-inc/chonkie)\n",
    "- [Google Gemini API](https://ai.google.dev/gemini-api/docs)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Experiment with different embeddings (OpenAI, Sentence Transformers)\n",
    "2. Integrate with vector databases (Chroma, Pinecone, Weaviate)\n",
    "3. Build a complete RAG application\n",
    "4. Optimize for your specific domain\n",
    "\n",
    "Thank you for completing this tutorial! Happy chunking!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
