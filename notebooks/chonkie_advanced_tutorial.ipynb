{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chonkie Advanced Tutorial: Refineries, Porters & Handshakes\n",
    "\n",
    "## Master Advanced Features for Production RAG Systems\n",
    "\n",
    "Welcome to the advanced Chonkie tutorial! This notebook covers the powerful features that take your RAG pipelines from prototype to production.\n",
    "\n",
    "### Prerequisites\n",
    "- Complete the [Chonkie Chunkers Tutorial](chonkie_complete_tutorial.ipynb) first\n",
    "- Understanding of chunking strategies\n",
    "- Google Gemini API key\n",
    "- Pinecone API key (free tier available)\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Refineries** - Enhance chunks with context and embeddings\n",
    "2. **Porters** - Export chunks to JSON and HuggingFace Datasets\n",
    "3. **Handshakes** - Integrate with vector databases (ChromaDB, Pinecone)\n",
    "4. **Production Pipelines** - Build complete end-to-end RAG systems\n",
    "\n",
    "### Tutorial Structure (60-90 minutes)\n",
    "\n",
    "1. Introduction & Setup (10 min)\n",
    "2. Refineries - Enhancing Chunks (25 min)\n",
    "3. Porters - Exporting Data (15 min)\n",
    "4. Handshakes - Vector DB Integration (30 min)\n",
    "5. Best Practices (10 min)\n",
    "6. Exercises (5 min)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction & Setup\n",
    "\n",
    "### 1.1 Understanding the RAG Pipeline\n",
    "\n",
    "In the previous tutorial, we learned about chunking. Now we'll complete the RAG pipeline:\n",
    "\n",
    "```\n",
    "Documents → Chunkers → Refineries → Porters/Handshakes → Vector DB → RAG\n",
    "```\n",
    "\n",
    "**Refineries** add context or embeddings to chunks\n",
    "**Porters** export chunks to various formats  \n",
    "**Handshakes** connect chunks directly to vector databases\n",
    "\n",
    "### When to Use These Features\n",
    "\n",
    "- **Overlap Refinery**: Prevent context loss at chunk boundaries\n",
    "- **Embeddings Refinery**: Pre-compute embeddings for batch operations\n",
    "- **JSONPorter**: Backup, debugging, version control\n",
    "- **DatasetsPorter**: Share data, ML workflows, HuggingFace Hub\n",
    "- **Handshakes**: Production RAG with scalable vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Setup and Imports\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import chonkie\n",
    "    print(f\"Chonkie version: {chonkie.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: Chonkie not installed. Run: pip install -r requirements.txt\")\n",
    "\n",
    "# Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\\nAll base libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Load API Keys\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Check Gemini API\n",
    "if not GEMINI_API_KEY or GEMINI_API_KEY == \"your_api_key_here\":\n",
    "    print(\"WARNING: Gemini API key not configured!\")\n",
    "    print(\"Set GEMINI_API_KEY in .env file\")\n",
    "else:\n",
    "    print(\"✓ Gemini API key loaded\")\n",
    "\n",
    "# Check Pinecone API\n",
    "if not PINECONE_API_KEY or PINECONE_API_KEY == \"your_pinecone_api_key_here\":\n",
    "    print(\"WARNING: Pinecone API key not configured!\")\n",
    "    print(\"Set PINECONE_API_KEY in .env file\")\n",
    "    print(\"Get key from: https://www.pinecone.io/\")\n",
    "else:\n",
    "    print(\"✓ Pinecone API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Import Chonkie Components\n",
    "try:\n",
    "    from chonkie import (\n",
    "        RecursiveChunker,\n",
    "        TokenChunker,\n",
    "        OverlapRefinery,\n",
    "        EmbeddingsRefinery,\n",
    "        JSONPorter,\n",
    "        DatasetsPorter,\n",
    "        ChromaHandshake,\n",
    "        PineconeHandshake,\n",
    "        GeminiEmbeddings,\n",
    "        Pipeline\n",
    "    )\n",
    "    print(\"✓ All Chonkie components imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR importing Chonkie components: {e}\")\n",
    "    print(\"Install missing dependencies: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Load Sample Data (from chunkers tutorial)\n",
    "try:\n",
    "    # Load technical documentation\n",
    "    with open(\"../data/sample_technical_doc.txt\", \"r\") as f:\n",
    "        technical_doc_text = f.read()\n",
    "\n",
    "    # Load research paper\n",
    "    with open(\"../data/sample_research_paper.txt\", \"r\") as f:\n",
    "        research_paper_text = f.read()\n",
    "\n",
    "    print(f\"✓ Technical Doc: {len(technical_doc_text):,} characters\")\n",
    "    print(f\"✓ Research Paper: {len(research_paper_text):,} characters\")\n",
    "    \n",
    "    print(\"\\n--- Sample Text Preview ---\")\n",
    "    print(technical_doc_text[:250] + \"...\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Sample data files not found!\")\n",
    "    print(\"Make sure you're running from the notebooks/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Initialize Gemini Embeddings\n",
    "try:\n",
    "    embeddings = GeminiEmbeddings(\n",
    "        model=\"gemini-embedding-exp-03-07\",\n",
    "        api_key=GEMINI_API_KEY,\n",
    "        task_type=\"SEMANTIC_SIMILARITY\"\n",
    "    )\n",
    "    \n",
    "    # Test embeddings\n",
    "    test_vector = embeddings.embed(\"Hello, this is a test\")\n",
    "    print(f\"✓ Gemini embeddings initialized\")\n",
    "    print(f\"  Embedding dimension: {len(test_vector)}\")\n",
    "    print(f\"  Sample values: {test_vector[:3]}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize Gemini embeddings: {e}\")\n",
    "    print(\"Some features will not work without valid API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7 Create Initial Chunks for Demonstrations\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Create chunks using RecursiveChunker\n",
    "chunker = RecursiveChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=128\n",
    ")\n",
    "\n",
    "# Chunk both documents\n",
    "tech_chunks = chunker.chunk(technical_doc_text)\n",
    "research_chunks = chunker.chunk(research_paper_text)\n",
    "\n",
    "print(f\"Created {len(tech_chunks)} chunks from technical doc\")\n",
    "print(f\"Created {len(research_chunks)} chunks from research paper\")\n",
    "print(f\"\\nTotal chunks for demonstrations: {len(tech_chunks) + len(research_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Refineries - Enhancing Chunks\n",
    "\n",
    "### 2.1 Understanding Refineries\n",
    "\n",
    "**Refineries** post-process chunks to add additional context or metadata. Think of them as enhancement layers that make chunks more useful for RAG.\n",
    "\n",
    "**Two Types:**\n",
    "1. **OverlapRefinery** - Adds overlapping context from adjacent chunks\n",
    "2. **EmbeddingsRefinery** - Attaches vector embeddings to chunk objects\n",
    "\n",
    "**Pipeline Position:**\n",
    "```\n",
    "Text → Chunker → Refinery → Porter/Handshake\n",
    "```\n",
    "\n",
    "**Why Refine?**\n",
    "- Prevent information loss at chunk boundaries\n",
    "- Pre-compute embeddings for efficiency\n",
    "- Enrich chunks with additional metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 OverlapRefinery - Adding Contextual Overlap\n",
    "\n",
    "**Purpose:** Add overlapping text from adjacent chunks to preserve context at boundaries\n",
    "\n",
    "**Use Cases:**\n",
    "- Narratives or stories where flow matters\n",
    "- Dialogues with speaker attribution\n",
    "- Technical docs with cross-references\n",
    "\n",
    "**How it works:**\n",
    "- Takes chunks and adds context from previous/next chunks\n",
    "- Configurable overlap size\n",
    "- Improves embedding quality at chunk edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OverlapRefinery\n",
    "overlap_refinery = OverlapRefinery(\n",
    "    tokenizer=\"gpt2\",  # Optional: specify tokenizer\n",
    "    context_size=128   # Overlap size in tokens\n",
    ")\n",
    "\n",
    "# Refine chunks with overlap\n",
    "refined_chunks = overlap_refinery(tech_chunks[:5])  # Use first 5 for demo\n",
    "\n",
    "print(f\"Refined {len(refined_chunks)} chunks with overlap context\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before and after\n",
    "print(\"COMPARISON: Original vs Refined Chunks\\n\")\n",
    "\n",
    "for i in range(min(2, len(refined_chunks))):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"\\nOriginal (no context):\")\n",
    "    print(f\"Text: {tech_chunks[i].text[:200]}...\")\n",
    "    print(f\"Length: {len(tech_chunks[i].text)} chars\")\n",
    "    \n",
    "    print(f\"\\nRefined (with overlap context):\")\n",
    "    print(f\"Text: {refined_chunks[i].text[:200]}...\")\n",
    "    print(f\"Length: {len(refined_chunks[i].text)} chars\")\n",
    "    \n",
    "    # Check for context attribute\n",
    "    if hasattr(refined_chunks[i], 'context'):\n",
    "        print(f\"Added context: {refined_chunks[i].context[:150]}...\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize size increase\n",
    "original_sizes = [len(chunk.text) for chunk in tech_chunks[:5]]\n",
    "refined_sizes = [len(chunk.text) for chunk in refined_chunks]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(original_sizes))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, original_sizes, width, label='Original', alpha=0.8)\n",
    "ax.bar(x + width/2, refined_sizes, width, label='With Overlap', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Chunk Index')\n",
    "ax.set_ylabel('Character Count')\n",
    "ax.set_title('Chunk Size: Original vs Refined with Overlap')\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage size increase: {np.mean(refined_sizes) - np.mean(original_sizes):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 EmbeddingsRefinery - Pre-Computing Embeddings\n",
    "\n",
    "**Purpose:** Attach vector embeddings directly to chunk objects\n",
    "\n",
    "**Use Cases:**\n",
    "- Batch embedding for multiple vector databases\n",
    "- Offline embedding computation\n",
    "- Caching embeddings for reuse\n",
    "\n",
    "**Benefits:**\n",
    "- Compute once, use many times\n",
    "- Separate embedding from storage\n",
    "- Enable custom embedding workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EmbeddingsRefinery\n",
    "embeddings_refinery = EmbeddingsRefinery(\n",
    "    embedding_model=embeddings  # Use our Gemini embeddings\n",
    ")\n",
    "\n",
    "# Refine chunks with embeddings (use subset to save API calls)\n",
    "sample_chunks = tech_chunks[:3]\n",
    "embedded_chunks = embeddings_refinery(sample_chunks)\n",
    "\n",
    "print(f\"✓ Added embeddings to {len(embedded_chunks)} chunks\")\n",
    "print(\"\\nNote: This makes API calls to Gemini - processing limited sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect embedded chunks\n",
    "for i, chunk in enumerate(embedded_chunks[:2]):\n",
    "    print(f\"\\n--- Embedded Chunk {i+1} ---\")\n",
    "    print(f\"Text: {chunk.text[:150]}...\")\n",
    "    \n",
    "    if hasattr(chunk, 'embedding'):\n",
    "        print(f\"\\n✓ Embedding attached!\")\n",
    "        print(f\"  Dimension: {len(chunk.embedding)}\")\n",
    "        print(f\"  Sample values: {chunk.embedding[:5]}\")\n",
    "        print(f\"  Type: {type(chunk.embedding)}\")\n",
    "    else:\n",
    "        print(\"No embedding attribute found\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory footprint analysis\n",
    "import sys\n",
    "\n",
    "# Estimate memory for chunk with embedding\n",
    "if embedded_chunks and hasattr(embedded_chunks[0], 'embedding'):\n",
    "    text_size = sys.getsizeof(embedded_chunks[0].text)\n",
    "    embedding_size = sys.getsizeof(embedded_chunks[0].embedding)\n",
    "    \n",
    "    print(\"Memory Footprint per Chunk:\")\n",
    "    print(f\"  Text: ~{text_size} bytes\")\n",
    "    print(f\"  Embedding: ~{embedding_size} bytes\")\n",
    "    print(f\"  Total: ~{text_size + embedding_size} bytes\")\n",
    "    print(f\"\\nFor 1000 chunks: ~{(text_size + embedding_size) * 1000 / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Chaining Refineries with Pipeline\n",
    "\n",
    "**The Power of Pipelines:**\n",
    "Chonkie's `Pipeline` API lets you chain chunkers and refineries elegantly:\n",
    "\n",
    "```python\n",
    "Pipeline()\n",
    "  .chunk_with(\"recursive\", chunk_size=512)\n",
    "  .refine_with(\"overlap\", context_size=128)\n",
    "  .refine_with(\"embeddings\", embedding_model=embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete pipeline\n",
    "pipeline = (\n",
    "    Pipeline()\n",
    "    .chunk_with(\"recursive\", tokenizer=\"gpt2\", chunk_size=512)\n",
    "    .refine_with(\"overlap\", context_size=128)\n",
    "    .refine_with(\"embeddings\", embedding_model=embeddings)\n",
    ")\n",
    "\n",
    "print(\"✓ Pipeline created with:\")\n",
    "print(\"  1. RecursiveChunker (512 tokens)\")\n",
    "print(\"  2. OverlapRefinery (128 token overlap)\")\n",
    "print(\"  3. EmbeddingsRefinery (Gemini embeddings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on sample text (limited for API costs)\n",
    "sample_text = research_paper_text[:2000]  # First 2000 chars\n",
    "\n",
    "print(\"Running pipeline...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline_result = pipeline.run(texts=sample_text)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Pipeline completed in {elapsed:.2f} seconds\")\n",
    "print(f\"✓ Produced {len(pipeline_result)} fully processed chunks\")\n",
    "print(\"  - Chunked\")\n",
    "print(\"  - Overlap context added\")\n",
    "print(\"  - Embeddings computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Build Your Own Pipeline\n",
    "\n",
    "Try creating a pipeline with different settings:\n",
    "- Different chunk sizes (256, 1024)\n",
    "- Different overlap sizes (64, 256)\n",
    "- With or without embeddings\n",
    "\n",
    "Compare the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom pipeline here\n",
    "# Example:\n",
    "# my_pipeline = (\n",
    "#     Pipeline()\n",
    "#     .chunk_with(\"token\", chunk_size=256)\n",
    "#     .refine_with(\"overlap\", context_size=64)\n",
    "# )\n",
    "# result = my_pipeline.run(texts=sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Porters - Exporting Chunks\n",
    "\n",
    "### 3.1 Understanding Porters\n",
    "\n",
    "**Porters** export chunks to various formats and destinations. They're the final step before storage or sharing.\n",
    "\n",
    "**Available Porters:**\n",
    "1. **JSONPorter** - Export to JSON files\n",
    "2. **DatasetsPorter** - Export to HuggingFace Datasets\n",
    "\n",
    "**When to Use:**\n",
    "- **JSONPorter**: Backup, debugging, version control, simple integrations\n",
    "- **DatasetsPorter**: ML workflows, data sharing, HuggingFace Hub uploads\n",
    "\n",
    "**Common Workflow:**\n",
    "```\n",
    "Chunk → Refine → Export (Porter) → Store/Share\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 JSONPorter - Exporting to JSON\n",
    "\n",
    "**Purpose:** Save chunks as structured JSON files\n",
    "\n",
    "**Benefits:**\n",
    "- Human-readable format\n",
    "- Universal compatibility\n",
    "- Easy to inspect and debug\n",
    "- Version control friendly\n",
    "\n",
    "**Output Structure:**\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"text\": \"chunk content...\",\n",
    "    \"token_count\": 487,\n",
    "    \"chunk_id\": \"chunk_0\",\n",
    "    \"embedding\": [0.123, ...],\n",
    "    \"metadata\": {...}\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize JSONPorter\n",
    "json_porter = JSONPorter()\n",
    "\n",
    "# Export embedded chunks to JSON\n",
    "output_path = \"../data/sample_chunks_output.json\"\n",
    "\n",
    "try:\n",
    "    json_porter.write(\n",
    "        chunks=embedded_chunks,\n",
    "        filepath=output_path\n",
    "    )\n",
    "    print(f\"✓ Exported {len(embedded_chunks)} chunks to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting to JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect exported JSON\n",
    "try:\n",
    "    with open(output_path, \"r\") as f:\n",
    "        exported_data = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(exported_data)} chunks from JSON\")\n",
    "    print(f\"\\nFile size: {os.path.getsize(output_path):,} bytes\")\n",
    "    \n",
    "    # Inspect structure\n",
    "    if exported_data:\n",
    "        print(f\"\\nChunk structure (keys): {list(exported_data[0].keys())}\")\n",
    "        print(f\"\\nFirst chunk preview:\")\n",
    "        print(json.dumps(exported_data[0], indent=2)[:500] + \"...\")\nexcept FileNotFoundError:\n",
    "    print(f\"File not found: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 DatasetsPorter - Exporting to HuggingFace Datasets\n",
    "\n",
    "**Purpose:** Convert chunks to HuggingFace Dataset format\n",
    "\n",
    "**Benefits:**\n",
    "- Rich dataset features (filtering, mapping, batching)\n",
    "- Direct upload to HuggingFace Hub\n",
    "- Integration with transformers ecosystem\n",
    "- Efficient storage and loading\n",
    "\n",
    "**Use Cases:**\n",
    "- Training embedding models\n",
    "- Sharing preprocessed data\n",
    "- Research and collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DatasetsPorter\n",
    "try:\n",
    "    datasets_porter = DatasetsPorter()\n",
    "    \n",
    "    # Export to HuggingFace Dataset\n",
    "    dataset = datasets_porter.write(chunks=embedded_chunks)\n",
    "    \n",
    "    print(f\"✓ Created HuggingFace Dataset with {len(dataset)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataset: {e}\")\n",
    "    print(\"Make sure datasets library is installed: pip install datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset\n",
    "if 'dataset' in locals():\n",
    "    print(\"Dataset Information:\")\n",
    "    print(f\"  Features: {dataset.features}\")\n",
    "    print(f\"  Number of rows: {len(dataset)}\")\n",
    "    print(f\"  Number of columns: {len(dataset.column_names)}\")\n",
    "    print(f\"\\nColumn names: {dataset.column_names}\")\n",
    "    \n",
    "    # Show first row\n",
    "    print(f\"\\nFirst row:\")\n",
    "    print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Push to HuggingFace Hub (commented out)\n",
    "# Uncomment and configure to upload your processed chunks\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login()  # Login to HuggingFace\n",
    "# \n",
    "# dataset.push_to_hub(\n",
    "#     repo_id=\"your-username/chonkie-tutorial-chunks\",\n",
    "#     private=True  # Set to False to make public\n",
    "# )\n",
    "\n",
    "print(\"Tip: You can push datasets to HuggingFace Hub for sharing!\")\n",
    "print(\"See: dataset.push_to_hub('username/dataset-name')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Porter Comparison\n",
    "\n",
    "Let's compare both porters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Feature': ['Format', 'File Size', 'Portability', 'HuggingFace Hub', 'ML Integration', 'Human Readable', 'Best For'],\n",
    "    'JSONPorter': ['JSON', 'Larger', 'Universal', 'No', 'Basic', 'Yes', 'Debugging, backup'],\n",
    "    'DatasetsPorter': ['HF Dataset', 'Smaller*', 'HF ecosystem', 'Yes', 'Excellent', 'Partial', 'ML workflows, sharing']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nPorter Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n*With Arrow format compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Export and Compare\n",
    "\n",
    "Export your chunks with both porters and compare:\n",
    "1. File sizes\n",
    "2. Loading speed\n",
    "3. Ease of inspection\n",
    "\n",
    "Which would you use for your project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Handshakes - Vector Database Integration\n",
    "\n",
    "### 4.1 Understanding Handshakes\n",
    "\n",
    "**Handshakes** provide a unified interface to connect Chonkie chunks with vector databases.\n",
    "\n",
    "**What They Do:**\n",
    "- Embed chunks (if not already embedded)\n",
    "- Store in vector database\n",
    "- Provide search interface\n",
    "- Handle batch operations\n",
    "\n",
    "**Supported Databases:**\n",
    "- ChromaDB (local/persistent)\n",
    "- Pinecone (managed cloud)\n",
    "- Qdrant, Weaviate, Milvus\n",
    "- MongoDB, Elasticsearch\n",
    "- Pgvector, Turbopuffer\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Text → Chunker → Refinery → Handshake → Vector DB\n",
    "                                ↓\n",
    "                         (Embed + Store)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ChromaDB Handshake - Local Development\n",
    "\n",
    "**Purpose:** Fast, local vector database for development and prototyping\n",
    "\n",
    "**Benefits:**\n",
    "- No API keys required\n",
    "- In-memory or persistent\n",
    "- Easy to setup\n",
    "- Great for testing\n",
    "\n",
    "**Limitations:**\n",
    "- Not for production scale\n",
    "- Single machine only\n",
    "- Limited advanced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB Handshake\n",
    "try:\n",
    "    chroma_handshake = ChromaHandshake(\n",
    "        collection_name=\"chonkie_tutorial\",\n",
    "        embedding_model=embeddings,\n",
    "        persist_directory=\"./chroma_db\"  # Optional: persist to disk\n",
    "    )\n",
    "    \n",
    "    print(\"✓ ChromaDB Handshake initialized\")\n",
    "    print(f\"  Collection: chonkie_tutorial\")\n",
    "    print(f\"  Persist dir: ./chroma_db\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing ChromaDB: {e}\")\n",
    "    print(\"Install with: pip install chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write chunks to ChromaDB\n",
    "if 'chroma_handshake' in locals():\n",
    "    try:\n",
    "        # Use our embedded chunks\n",
    "        chroma_handshake.write(embedded_chunks)\n",
    "        print(f\"✓ Wrote {len(embedded_chunks)} chunks to ChromaDB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to ChromaDB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search ChromaDB\n",
    "if 'chroma_handshake' in locals():\n",
    "    try:\n",
    "        query = \"What are the best practices for chunking in RAG?\"\n",
    "        results = chroma_handshake.search(\n",
    "            query=query,\n",
    "            limit=3\n",
    "        )\n",
    "        \n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"\\nTop {len(results)} Results:\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"\\nResult {i+1}:\")\n",
    "            print(f\"Score: {result.get('score', 'N/A')}\")\n",
    "            print(f\"Text: {result['text'][:200]}...\")\n",
    "            print(\"-\" * 80)\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching ChromaDB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Pinecone Handshake - Production Scale\n",
    "\n",
    "**Purpose:** Managed, cloud-native vector database for production\n",
    "\n",
    "**Benefits:**\n",
    "- Fully managed (no infrastructure)\n",
    "- Scales to billions of vectors\n",
    "- Low latency (<10ms)\n",
    "- Advanced features (namespaces, metadata filtering)\n",
    "- Free tier available\n",
    "\n",
    "**Perfect For:**\n",
    "- Production RAG applications\n",
    "- Large-scale systems\n",
    "- Multi-tenant applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "try:\n",
    "    import pinecone\n",
    "    \n",
    "    # Initialize Pinecone client\n",
    "    pinecone.init(\n",
    "        api_key=PINECONE_API_KEY,\n",
    "        environment=\"us-west1-gcp\"  # Change to your environment\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Pinecone initialized\")\n",
    "    print(f\"  Environment: us-west1-gcp\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Pinecone: {e}\")\n",
    "    print(\"Make sure PINECONE_API_KEY is set in .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pinecone index (one-time setup)\n",
    "if 'pinecone' in dir():\n",
    "    try:\n",
    "        index_name = \"chonkie-tutorial\"\n",
    "        \n",
    "        # Check if index exists\n",
    "        if index_name not in pinecone.list_indexes():\n",
    "            print(f\"Creating index '{index_name}'...\")\n",
    "            pinecone.create_index(\n",
    "                name=index_name,\n",
    "                dimension=768,  # Match embedding dimension\n",
    "                metric=\"cosine\"\n",
    "            )\n",
    "            print(\"✓ Index created\")\n",
    "        else:\n",
    "            print(f\"✓ Index '{index_name}' already exists\")\n",
    "        \n",
    "        # Get index stats\n",
    "        index = pinecone.Index(index_name)\n",
    "        stats = index.describe_index_stats()\n",
    "        print(f\"\\nIndex stats:\")\n",
    "        print(f\"  Total vectors: {stats.get('total_vector_count', 0)}\")\n",
    "        print(f\"  Dimension: {stats.get('dimension', 'N/A')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Pinecone index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone Handshake\n",
    "if 'pinecone' in dir() and 'index_name' in locals():\n",
    "    try:\n",
    "        pinecone_handshake = PineconeHandshake(\n",
    "            index_name=index_name,\n",
    "            embedding_model=embeddings,\n",
    "            namespace=\"tutorial_docs\"  # Optional namespace\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Pinecone Handshake initialized\")\n",
    "        print(f\"  Index: {index_name}\")\n",
    "        print(f\"  Namespace: tutorial_docs\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone Handshake: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write chunks to Pinecone\n",
    "if 'pinecone_handshake' in locals():\n",
    "    try:\n",
    "        print(\"Writing chunks to Pinecone...\")\n",
    "        pinecone_handshake.write(\n",
    "            chunks=embedded_chunks,\n",
    "            batch_size=100  # Upload in batches\n",
    "        )\n",
    "        print(f\"✓ Wrote {len(embedded_chunks)} chunks to Pinecone\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to Pinecone: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Pinecone with metadata filtering\n",
    "if 'pinecone_handshake' in locals():\n",
    "    try:\n",
    "        query = \"semantic chunking strategies\"\n",
    "        \n",
    "        results = pinecone_handshake.search(\n",
    "            query=query,\n",
    "            limit=5,\n",
    "            # filter={\"source\": \"research_paper\"}  # Optional metadata filter\n",
    "        )\n",
    "        \n",
    "        print(f\"Query: '{query}'\")\n",
    "        print(f\"\\nTop {len(results)} Results from Pinecone:\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"\\nResult {i+1}:\")\n",
    "            print(f\"Score: {result.get('score', 'N/A'):.3f}\")\n",
    "            print(f\"Text: {result['text'][:200]}...\")\n",
    "            if 'metadata' in result:\n",
    "                print(f\"Metadata: {result['metadata']}\")\n",
    "            print(\"-\" * 80)\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching Pinecone: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Complete RAG Workflow Example\n",
    "\n",
    "Let's build an end-to-end RAG system combining everything we've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAG Pipeline Function\n",
    "def build_rag_system(documents, vector_db=\"pinecone\"):\n",
    "    \"\"\"\n",
    "    Build complete RAG system from documents.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of text documents\n",
    "        vector_db: \"chroma\" or \"pinecone\"\n",
    "    \n",
    "    Returns:\n",
    "        Handshake object for querying\n",
    "    \"\"\"\n",
    "    print(\"Building RAG System...\\n\")\n",
    "    \n",
    "    # Step 1: Create pipeline\n",
    "    print(\"1. Creating processing pipeline...\")\n",
    "    pipeline = (\n",
    "        Pipeline()\n",
    "        .chunk_with(\"recursive\", chunk_size=512)\n",
    "        .refine_with(\"overlap\", context_size=128)\n",
    "        .refine_with(\"embeddings\", embedding_model=embeddings)\n",
    "    )\n",
    "    print(\"   ✓ Pipeline ready\\n\")\n",
    "    \n",
    "    # Step 2: Process documents\n",
    "    print(\"2. Processing documents...\")\n",
    "    all_chunks = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        chunks = pipeline.run(texts=doc)\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"   ✓ Document {i+1}: {len(chunks)} chunks\")\n",
    "    print(f\"   Total: {len(all_chunks)} chunks\\n\")\n",
    "    \n",
    "    # Step 3: Export backup\n",
    "    print(\"3. Exporting backup to JSON...\")\n",
    "    json_porter = JSONPorter()\n",
    "    json_porter.write(all_chunks, \"../data/rag_system_backup.json\")\n",
    "    print(\"   ✓ Backup saved\\n\")\n",
    "    \n",
    "    # Step 4: Store in vector DB\n",
    "    print(f\"4. Storing in {vector_db}...\")\n",
    "    if vector_db == \"chroma\":\n",
    "        handshake = ChromaHandshake(\n",
    "            collection_name=\"rag_system\",\n",
    "            embedding_model=embeddings\n",
    "        )\n",
    "    else:  # pinecone\n",
    "        handshake = PineconeHandshake(\n",
    "            index_name=index_name,\n",
    "            embedding_model=embeddings,\n",
    "            namespace=\"rag_system\"\n",
    "        )\n",
    "    \n",
    "    handshake.write(all_chunks)\n",
    "    print(f\"   ✓ {len(all_chunks)} chunks stored\\n\")\n",
    "    \n",
    "    print(\"✓ RAG System ready!\")\n",
    "    return handshake\n",
    "\n",
    "print(\"RAG system builder function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG system (using limited sample to save API costs)\n",
    "sample_docs = [\n",
    "    technical_doc_text[:1500],\n",
    "    research_paper_text[:1500]\n",
    "]\n",
    "\n",
    "# Choose vector DB\n",
    "rag_handshake = build_rag_system(sample_docs, vector_db=\"chroma\")  # Use \"pinecone\" if preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the RAG system\n",
    "def rag_query(handshake, question, k=3):\n",
    "    \"\"\"\n",
    "    Query the RAG system.\n",
    "    \n",
    "    Args:\n",
    "        handshake: Vector DB handshake\n",
    "        question: User query\n",
    "        k: Number of chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Retrieved context and answer\n",
    "    \"\"\"\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    results = handshake.search(query=question, limit=k)\n",
    "    \n",
    "    # Combine context\n",
    "    context = \"\\n\\n\".join([r['text'] for r in results])\n",
    "    \n",
    "    print(f\"Retrieved {len(results)} relevant chunks:\\n\")\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"Chunk {i+1} (score: {r.get('score', 'N/A')})\")\n",
    "        print(f\"{r['text'][:150]}...\\n\")\n",
    "    \n",
    "    # In production, you'd send context to LLM for answer generation\n",
    "    answer = f\"Based on {k} retrieved chunks, the answer would be generated by an LLM using this context.\"\n",
    "    \n",
    "    return context, answer\n",
    "\n",
    "# Test query\n",
    "context, answer = rag_query(\n",
    "    rag_handshake,\n",
    "    \"How does recursive chunking work?\",\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"\\nNext step: Send context to LLM (GPT-4, Gemini, etc.) for answer generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Handshakes Comparison\n",
    "\n",
    "Here's how different vector databases compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "handshake_comparison = {\n",
    "    'Database': ['ChromaDB', 'Pinecone', 'Qdrant', 'Weaviate'],\n",
    "    'Deployment': ['Local/Self-hosted', 'Managed Cloud', 'Both', 'Both'],\n",
    "    'Best For': ['Development', 'Production', 'Balance', 'GraphQL/Hybrid'],\n",
    "    'API Required': ['No', 'Yes', 'Optional', 'Optional'],\n",
    "    'Cost': ['Free', 'Free tier + Paid', 'Free + Paid', 'Free + Paid'],\n",
    "    'Scale': ['Medium', 'Billions', 'Billions', 'Large'],\n",
    "    'Setup Difficulty': ['Easy', 'Easy', 'Medium', 'Medium']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(handshake_comparison)\n",
    "print(\"\\nVector Database Comparison:\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Best Practices & Integration\n",
    "\n",
    "### 5.1 When to Use Each Component\n",
    "\n",
    "**Decision Matrix:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision guide\n",
    "decision_guide = {\n",
    "    'Scenario': [\n",
    "        'Story/narrative chunking',\n",
    "        'Batch embedding job',\n",
    "        'Data backup needed',\n",
    "        'Share with team',\n",
    "        'Local development',\n",
    "        'Production RAG',\n",
    "        'Self-hosted required',\n",
    "        'ML model training'\n",
    "    ],\n",
    "    'Recommended': [\n",
    "        'OverlapRefinery',\n",
    "        'EmbeddingsRefinery',\n",
    "        'JSONPorter',\n",
    "        'DatasetsPorter',\n",
    "        'ChromaDB Handshake',\n",
    "        'Pinecone Handshake',\n",
    "        'Qdrant/Weaviate',\n",
    "        'DatasetsPorter'\n",
    "    ],\n",
    "    'Why': [\n",
    "        'Preserves context at boundaries',\n",
    "        'Compute once, reuse many times',\n",
    "        'Human-readable, version control',\n",
    "        'HuggingFace Hub integration',\n",
    "        'No setup, no API keys',\n",
    "        'Scalable, managed, fast',\n",
    "        'Control, privacy, customization',\n",
    "        'HF ecosystem integration'\n",
    "    ]\n",
    "}\n",
    "\n",
    "guide_df = pd.DataFrame(decision_guide)\n",
    "print(\"\\nDecision Guide: What to Use When\")\n",
    "print(\"=\" * 100)\n",
    "print(guide_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Production Pipeline Pattern\n",
    "\n",
    "Here's a robust production pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_pipeline(documents, config):\n",
    "    \"\"\"\n",
    "    Production-ready RAG pipeline with error handling.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of text documents\n",
    "        config: Configuration dict with settings\n",
    "    \n",
    "    Returns:\n",
    "        dict with results and statistics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'chunks_processed': 0,\n",
    "        'errors': [],\n",
    "        'timing': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 1. Chunk with appropriate strategy\n",
    "        start = time.time()\n",
    "        chunker = RecursiveChunker(\n",
    "            chunk_size=config.get('chunk_size', 512),\n",
    "            chunk_overlap=config.get('overlap', 128)\n",
    "        )\n",
    "        chunks = chunker.chunk_batch(documents)\n",
    "        results['timing']['chunking'] = time.time() - start\n",
    "        results['chunks_processed'] = len(chunks)\n",
    "        \n",
    "        # 2. Refine for quality (if enabled)\n",
    "        if config.get('use_overlap_refinery', True):\n",
    "            start = time.time()\n",
    "            overlap_refinery = OverlapRefinery(\n",
    "                context_size=config.get('context_size', 128)\n",
    "            )\n",
    "            chunks = overlap_refinery(chunks)\n",
    "            results['timing']['overlap_refinery'] = time.time() - start\n",
    "        \n",
    "        # 3. Add embeddings (with retry logic)\n",
    "        if config.get('use_embeddings', True):\n",
    "            start = time.time()\n",
    "            embeddings_refinery = EmbeddingsRefinery(\n",
    "                embedding_model=config['embedding_model']\n",
    "            )\n",
    "            chunks = embeddings_refinery(chunks)\n",
    "            results['timing']['embeddings'] = time.time() - start\n",
    "        \n",
    "        # 4. Export backup (async in production)\n",
    "        if config.get('export_backup', True):\n",
    "            start = time.time()\n",
    "            json_porter = JSONPorter()\n",
    "            backup_path = config.get('backup_path', 'backup/chunks.json')\n",
    "            json_porter.write(chunks, backup_path)\n",
    "            results['timing']['backup'] = time.time() - start\n",
    "        \n",
    "        # 5. Store in vector DB (with batch size limit)\n",
    "        start = time.time()\n",
    "        vector_db = config.get('vector_db', 'pinecone')\n",
    "        \n",
    "        if vector_db == 'pinecone':\n",
    "            handshake = PineconeHandshake(\n",
    "                index_name=config['index_name'],\n",
    "                embedding_model=config['embedding_model']\n",
    "            )\n",
    "        else:  # chroma\n",
    "            handshake = ChromaHandshake(\n",
    "                collection_name=config.get('collection_name', 'default'),\n",
    "                embedding_model=config['embedding_model']\n",
    "            )\n",
    "        \n",
    "        handshake.write(chunks, batch_size=config.get('batch_size', 100))\n",
    "        results['timing']['vector_db'] = time.time() - start\n",
    "        \n",
    "        results['status'] = 'success'\n",
    "        results['handshake'] = handshake\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['status'] = 'error'\n",
    "        results['errors'].append(str(e))\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Production pipeline function ready\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"  - Error handling\")\n",
    "print(\"  - Configurable components\")\n",
    "print(\"  - Performance timing\")\n",
    "print(\"  - Batch operations\")\n",
    "print(\"  - Backup integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Performance Considerations\n",
    "\n",
    "**Key Optimizations:**\n",
    "\n",
    "1. **Batch Processing**: Use `chunk_batch()` for multiple documents\n",
    "2. **Memory Management**: Process large corpora in chunks\n",
    "3. **API Costs**: Optimize chunk sizes to minimize embedding API calls\n",
    "4. **Database Costs**: Monitor vector DB usage and index size\n",
    "5. **Caching**: Reuse embeddings when possible\n",
    "6. **Parallel Processing**: Use async/threading for large batches\n",
    "\n",
    "**Cost Estimation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_costs(num_documents, avg_tokens_per_doc, chunk_size=512):\n",
    "    \"\"\"\n",
    "    Estimate embedding and storage costs.\n",
    "    \n",
    "    Args:\n",
    "        num_documents: Number of documents to process\n",
    "        avg_tokens_per_doc: Average tokens per document\n",
    "        chunk_size: Chunk size in tokens\n",
    "    \"\"\"\n",
    "    # Calculate chunks\n",
    "    total_tokens = num_documents * avg_tokens_per_doc\n",
    "    num_chunks = total_tokens / chunk_size\n",
    "    \n",
    "    # Embedding costs (example: Gemini pricing)\n",
    "    embedding_cost_per_1m = 0.025  # $0.025 per 1M tokens (example)\n",
    "    embedding_cost = (total_tokens / 1_000_000) * embedding_cost_per_1m\n",
    "    \n",
    "    # Storage costs (example: Pinecone pricing)\n",
    "    vectors_per_pod = 1_000_000\n",
    "    cost_per_pod = 70  # $70/month (example)\n",
    "    pods_needed = max(1, num_chunks / vectors_per_pod)\n",
    "    storage_cost_monthly = pods_needed * cost_per_pod\n",
    "    \n",
    "    print(\"Cost Estimation:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Documents: {num_documents:,}\")\n",
    "    print(f\"Total tokens: {total_tokens:,}\")\n",
    "    print(f\"Estimated chunks: {num_chunks:,.0f}\")\n",
    "    print(f\"\\nEmbedding cost (one-time): ${embedding_cost:.2f}\")\n",
    "    print(f\"Vector DB cost (monthly): ${storage_cost_monthly:.2f}\")\n",
    "    print(f\"Pods needed: {pods_needed:.1f}\")\n",
    "\n",
    "# Example estimation\n",
    "estimate_costs(\n",
    "    num_documents=10000,\n",
    "    avg_tokens_per_doc=2000,\n",
    "    chunk_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Exercises & Next Steps\n",
    "\n",
    "### Exercises\n",
    "\n",
    "Try these hands-on exercises to solidify your learning:\n",
    "\n",
    "1. **Custom Pipeline Builder**\n",
    "   - Build a pipeline with your preferred chunker, refineries, and porter\n",
    "   - Process your own documents\n",
    "   - Compare results with different configurations\n",
    "\n",
    "2. **Handshake Comparison**\n",
    "   - Index same data in ChromaDB and Pinecone\n",
    "   - Query both with identical questions\n",
    "   - Compare retrieval quality and speed\n",
    "\n",
    "3. **Cost Optimization**\n",
    "   - Experiment with different chunk sizes (256, 512, 1024)\n",
    "   - Calculate embedding costs for each\n",
    "   - Find optimal balance of quality vs cost\n",
    "\n",
    "4. **Production Setup**\n",
    "   - Design complete RAG architecture for a use case\n",
    "   - Choose chunker, refineries, and vector DB\n",
    "   - Document rationale for each choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your custom pipeline\n",
    "# Build and test your own pipeline configuration\n",
    "\n",
    "# my_pipeline = (\n",
    "#     Pipeline()\n",
    "#     .chunk_with(\"...\", ...)\n",
    "#     .refine_with(\"...\", ...)\n",
    "# )\n",
    "# my_result = my_pipeline.run(texts=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Compare handshakes\n",
    "# Query both ChromaDB and Pinecone, compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Refineries enhance chunks** - Add overlap for context, pre-compute embeddings\n",
    "2. **Porters enable data portability** - JSON for backup/debugging, Datasets for ML\n",
    "3. **Handshakes unify vector DB access** - Consistent API across 9+ databases\n",
    "4. **Pipeline composition is powerful** - Chain components for complete workflows\n",
    "5. **Choose tools based on needs** - Development vs production, cost vs quality\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Chonkie Documentation](https://docs.chonkie.ai/)\n",
    "- [Chonkie GitHub](https://github.com/chonkie-inc/chonkie)\n",
    "- [Pinecone Docs](https://docs.pinecone.io/)\n",
    "- [ChromaDB Docs](https://docs.trychroma.com/)\n",
    "- [HuggingFace Datasets](https://huggingface.co/docs/datasets/)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Integrate with your RAG application**\n",
    "   - Replace existing chunking logic\n",
    "   - Add refineries for better quality\n",
    "   - Switch to production vector DB\n",
    "\n",
    "2. **Experiment with vector databases**\n",
    "   - Try Qdrant, Weaviate, or others\n",
    "   - Compare performance and features\n",
    "   - Choose best fit for your needs\n",
    "\n",
    "3. **Build evaluation framework**\n",
    "   - Test retrieval quality\n",
    "   - Measure end-to-end performance\n",
    "   - Optimize based on metrics\n",
    "\n",
    "4. **Scale to production**\n",
    "   - Implement error handling\n",
    "   - Add monitoring and logging\n",
    "   - Optimize for cost and performance\n",
    "\n",
    "### Thank You!\n",
    "\n",
    "You've completed the advanced Chonkie tutorial! You now have the knowledge to build production-ready RAG systems with:\n",
    "\n",
    "- Advanced chunking strategies\n",
    "- Context enhancement with refineries\n",
    "- Data portability with porters\n",
    "- Scalable vector database integration\n",
    "\n",
    "Happy building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
